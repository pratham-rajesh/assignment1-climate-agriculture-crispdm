{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "50927b0c",
   "metadata": {},
   "source": [
    "# Climate Change Impact on Agriculture — CRISP‑DM (Professional Colab)\n",
    "**Scope:** End‑to‑end, compute‑aware CRISP‑DM walkthrough: EDA → Cleaning/Preprocessing → Feature Engineering (outlier flags) → Baselines & Models → Evaluation & Interpretability → Final Recommendation.  \n",
    "**Packages:** `pandas`, `numpy`, `scikit‑learn`, `matplotlib`, `seaborn` (optional `shap`).\n",
    "\n",
    "**Author’s note:** This notebook is designed for **Colab**. It includes upload/Drive/Kaggle options for data access and compute‑friendly toggles."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c6d095d",
   "metadata": {},
   "source": [
    "## Table of Contents\n",
    "1. [Environment & Config](#env)  \n",
    "2. [Data Access](#data)  \n",
    "3. [CRISP‑DM Step 1 — Business Understanding](#bu)  \n",
    "4. [CRISP‑DM Step 2 — Data Understanding (EDA)](#eda)  \n",
    "5. [CRISP‑DM Step 3 — Data Preparation](#prep)  \n",
    "6. [CRISP‑DM Step 4 — Baselines & Candidate Models](#models)  \n",
    "7. [CRISP‑DM Step 5 — Evaluation & Interpretability](#eval)  \n",
    "8. [CRISP‑DM Step 6 — Final Recommendation & Reporting](#final)  \n",
    "9. [Artifacts & Download](#artifacts)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1927c89f",
   "metadata": {},
   "source": [
    "## 1) Environment & Config <a id='env'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e655a0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Setup & compute-aware config\n",
    "import sys, os, math, warnings\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Optional visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from typing import List, Tuple, Dict\n",
    "\n",
    "pd.set_option(\"display.max_columns\", 120)\n",
    "pd.set_option(\"display.width\", 160)\n",
    "pd.options.display.float_format = \"{:,.4f}\".format\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# Reproducibility\n",
    "SEED = 42\n",
    "np.random.seed(SEED)\n",
    "\n",
    "# Compute-aware toggles\n",
    "FAST_MODE = True  # if True, use smaller subsamples and modest model sizes\n",
    "TRAIN_SUBSAMPLE = 3000 if FAST_MODE else None\n",
    "RF_TREES = 120 if FAST_MODE else 400\n",
    "GB_TREES = 120 if FAST_MODE else 500\n",
    "GB_LR    = 0.05\n",
    "GB_DEPTH = 3\n",
    "PERM_N_REPEATS = 3 if FAST_MODE else 10\n",
    "\n",
    "print(\"FAST_MODE:\", FAST_MODE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8068535",
   "metadata": {},
   "source": [
    "## 2) Data Access <a id='data'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21bfe6c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Data access helpers (Upload / Drive / URL / Kaggle)\n",
    "from pathlib import Path\n",
    "\n",
    "DATA_PATH = \"/content/climate_change_impact_on_agriculture_2024.csv\"  # change if needed\n",
    "\n",
    "def ensure_data(DATA_PATH=DATA_PATH):\n",
    "    p = Path(DATA_PATH)\n",
    "    if p.exists():\n",
    "        print(f\"Found dataset at {p}\")\n",
    "        return str(p)\n",
    "    # Try Google Colab upload\n",
    "    try:\n",
    "        from google.colab import files\n",
    "        print(\"Dataset not found. Please upload the CSV...\")\n",
    "        uploaded = files.upload()\n",
    "        for k in uploaded.keys():\n",
    "            if k.lower().endswith(\".csv\"):\n",
    "                os.rename(k, Path(DATA_PATH).name)\n",
    "                print(f\"Saved uploaded file as {Path(DATA_PATH).name}\")\n",
    "                return str(Path(DATA_PATH).name)\n",
    "        raise FileNotFoundError(\"No CSV uploaded.\")\n",
    "    except Exception as e:\n",
    "        print(\"Upload failed or not in Colab environment:\", repr(e))\n",
    "        raise\n",
    "\n",
    "DATA_PATH = ensure_data(DATA_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "627ba136",
   "metadata": {},
   "source": [
    "## 3) CRISP‑DM Step 1 — Business Understanding <a id='bu'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "441f5bf6",
   "metadata": {},
   "source": [
    "**Goal.** Predict **crop yield (t/ha)** from climate and context to support adaptation (variety choice, irrigation, risk management).  \n",
    "**Success.** Out‑of‑time RMSE materially better than a groupwise baseline; consistent with agronomic intuition; explainable via feature attributions.  \n",
    "**Risks.** Leakage, non‑stationarity, collinearity, aggregation bias. Mitigated with time‑aware evaluation, regularization, and robust preprocessing."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c4a630d",
   "metadata": {},
   "source": [
    "## 4) CRISP‑DM Step 2 — Data Understanding (EDA) <a id='eda'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c30750e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Load, schema, and quick summaries\n",
    "df = pd.read_csv(DATA_PATH)\n",
    "n_rows, n_cols = df.shape\n",
    "print(f\"Rows: {n_rows}, Cols: {n_cols}\")\n",
    "\n",
    "def normalize(s): return \"\".join(ch.lower() for ch in str(s) if ch.isalnum())\n",
    "colmap = {normalize(c): c for c in df.columns}\n",
    "\n",
    "def resolve(cands: List[str]):\n",
    "    for c in cands:\n",
    "        k = normalize(c)\n",
    "        if k in colmap: return colmap[k]\n",
    "    return None\n",
    "\n",
    "year_col    = resolve([\"Year\"])\n",
    "country_col = resolve([\"Country\"])\n",
    "region_col  = resolve([\"Region\"])\n",
    "crop_col    = resolve([\"Crop_Type\",\"Crop\"])\n",
    "temp_col    = resolve([\"Average_Temperature_C\"])\n",
    "precip_col  = resolve([\"Total_Precipitation_mm\"])\n",
    "co2_col     = resolve([\"CO2_Emissions_MT\",\"CO2_Emissions\"])\n",
    "yield_col   = resolve([\"Crop_Yield_MT_per_HA\",\"Crop_Yield_t_per_ha\",\"Yield\",\"Yield_MT_per_HA\"])\n",
    "\n",
    "if year_col:    df[year_col] = pd.to_numeric(df[year_col], errors=\"coerce\").astype(\"Int64\")\n",
    "for c in [temp_col, precip_col, co2_col, yield_col]:\n",
    "    if c: df[c] = pd.to_numeric(df[c], errors=\"coerce\")\n",
    "for c in [country_col, region_col, crop_col]:\n",
    "    if c: df[c] = df[c].astype(\"category\")\n",
    "\n",
    "display(df.head(10))\n",
    "display(df.dtypes.to_frame(\"dtype\"))\n",
    "\n",
    "numeric_cols = df.select_dtypes(include=[np.number, \"Int64\"]).columns.tolist()\n",
    "categorical_cols = df.select_dtypes(exclude=[np.number, \"Int64\"]).columns.tolist()\n",
    "\n",
    "print(\"Numeric:\", len(numeric_cols), \"Categorical:\", len(categorical_cols))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cfa6975",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Univariate plots (key numeric) & categorical bars\n",
    "key_numeric = [c for c in [temp_col, precip_col, co2_col, yield_col] if c]\n",
    "for col in key_numeric:\n",
    "    fig, ax = plt.subplots()\n",
    "    sns.histplot(df[col].dropna(), bins=30, ax=ax)\n",
    "    ax.set_title(f\"Histogram — {col}\")\n",
    "    plt.show()\n",
    "\n",
    "    fig, ax = plt.subplots()\n",
    "    sns.boxplot(x=df[col], ax=ax)\n",
    "    ax.set_title(f\"Boxplot — {col}\")\n",
    "    plt.show()\n",
    "\n",
    "for cat in [country_col, region_col, crop_col]:\n",
    "    if cat:\n",
    "        vc = df[cat].value_counts(dropna=False).head(15).sort_values(ascending=True)\n",
    "        fig, ax = plt.subplots()\n",
    "        ax.barh(vc.index.astype(str), vc.values)\n",
    "        ax.set_title(f\"Top 15 categories — {cat}\")\n",
    "        ax.set_xlabel(\"Count\")\n",
    "        plt.tight_layout(); plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3abc6bc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Correlation & climate→yield relationships\n",
    "corr = df[numeric_cols].corr(method=\"pearson\")\n",
    "fig, ax = plt.subplots(figsize=(10,7))\n",
    "sns.heatmap(corr, cmap=\"vlag\", ax=ax)\n",
    "ax.set_title(\"Correlation Heatmap — Numeric Features\")\n",
    "plt.show()\n",
    "\n",
    "if yield_col:\n",
    "    target_corr = corr[yield_col].sort_values(ascending=False)\n",
    "    display(target_corr.to_frame(\"corr_with_yield\").head(15))\n",
    "    for x in [temp_col, precip_col, co2_col]:\n",
    "        if x:\n",
    "            fig, ax = plt.subplots()\n",
    "            sns.scatterplot(data=df, x=x, y=yield_col, s=12, alpha=0.4, ax=ax)\n",
    "            ax.set_title(f\"{yield_col} vs {x}\")\n",
    "            plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7a18a80",
   "metadata": {},
   "source": [
    "**EDA summary.** Temperature shows a non‑linear effect (optimum/heat stress), precipitation saturates, and extreme‑event proxies likely matter. Yield distribution is mildly right‑skewed with meaningful outliers (kept and flagged). Categorical frequencies are imbalanced; we report groupwise errors later."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3901abf",
   "metadata": {},
   "source": [
    "## 5) CRISP‑DM Step 3 — Data Preparation <a id='prep'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01e37e83",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Missingness, leakage scan, and preprocessing pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import OneHotEncoder, RobustScaler\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "\n",
    "# Drop exact duplicates\n",
    "before = len(df); df = df.drop_duplicates(); after = len(df)\n",
    "print(\"drop_duplicates:\", before, \"->\", after)\n",
    "\n",
    "# Missingness table\n",
    "missing = (df.isna().sum().to_frame(\"missing\")\n",
    "           .assign(total=len(df))\n",
    "           .assign(pct=lambda d: 100*d[\"missing\"]/d[\"total\"])\n",
    "           .sort_values(\"pct\", ascending=False))\n",
    "display(missing.head(20))\n",
    "\n",
    "# Leakage detection\n",
    "leaky_cols = []\n",
    "for c in df.columns:\n",
    "    nm = c.lower()\n",
    "    if c == yield_col: \n",
    "        continue\n",
    "    if any(k in nm for k in [\"yield\", \"production\"]):\n",
    "        leaky_cols.append(c)\n",
    "# Keep area variables if present (pre-planting info)\n",
    "leaky_cols = [c for c in leaky_cols if \"area\" not in c.lower()]\n",
    "print(\"Leaky columns removed from features:\", leaky_cols)\n",
    "\n",
    "feature_cols = [c for c in df.columns if c not in [yield_col] + leaky_cols]\n",
    "num_cols = df[feature_cols].select_dtypes(include=[np.number, \"Int64\"]).columns.tolist()\n",
    "cat_cols = df[feature_cols].select_dtypes(exclude=[np.number, \"Int64\"]).columns.tolist()\n",
    "\n",
    "class OutlierFlagger(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, feature_names, methods=(\"mad\",\"iqr\"), mad_thresh=3.5, iqr_k=1.5, prefix=\"flag\"):\n",
    "        self.feature_names = feature_names; self.methods = methods\n",
    "        self.mad_thresh = mad_thresh; self.iqr_k = iqr_k; self.prefix = prefix\n",
    "    def fit(self, X, y=None):\n",
    "        X = np.asarray(X, dtype=float)\n",
    "        self.medians_ = np.nanmedian(X, axis=0)\n",
    "        self.mads_ = np.nanmedian(np.abs(X - self.medians_), axis=0)\n",
    "        self.q1_ = np.nanpercentile(X, 25, axis=0)\n",
    "        self.q3_ = np.nanpercentile(X, 75, axis=0)\n",
    "        self.iqr_ = self.q3_ - self.q1_\n",
    "        self.names_ = []\n",
    "        for fn in self.feature_names:\n",
    "            if \"mad\" in self.methods: self.names_.append(f\"{self.prefix}_{fn}_mad\")\n",
    "            if \"iqr\" in self.methods: self.names_.append(f\"{self.prefix}_{fn}_iqr\")\n",
    "        return self\n",
    "    def transform(self, X):\n",
    "        X = np.asarray(X, dtype=float); outs = []\n",
    "        for i in range(X.shape[1]):\n",
    "            col = X[:, i]\n",
    "            if \"mad\" in self.methods:\n",
    "                mad = self.mads_[i]; med = self.medians_[i]\n",
    "                flags_mad = np.zeros_like(col) if (mad==0 or np.isnan(mad)) else (np.abs(0.6745*(col-med)/mad)>self.mad_thresh).astype(float)\n",
    "                outs.append(flags_mad)\n",
    "            if \"iqr\" in self.methods:\n",
    "                q1, q3, iqr = self.q1_[i], self.q3_[i], self.iqr_[i]\n",
    "                lower, upper = q1- self.iqr_k*iqr, q3 + self.iqr_k*iqr\n",
    "                flags_iqr = ((col<lower)|(col>upper)).astype(float)\n",
    "                outs.append(flags_iqr)\n",
    "        return np.vstack(outs).T if outs else np.zeros((X.shape[0],0))\n",
    "    def get_feature_names_out(self, input_features=None):\n",
    "        return np.array(self.names_) if hasattr(self, \"names_\") else np.array([])\n",
    "\n",
    "numeric_pipeline = Pipeline([(\"imputer\", SimpleImputer(strategy=\"median\")),\n",
    "                             (\"scaler\", RobustScaler())])\n",
    "flags_pipeline   = Pipeline([(\"imputer\", SimpleImputer(strategy=\"median\")),\n",
    "                             (\"flags\", OutlierFlagger(feature_names=num_cols))])\n",
    "categorical_pipeline = Pipeline([(\"imputer\", SimpleImputer(strategy=\"constant\", fill_value=\"Unknown\")),\n",
    "                                 (\"ohe\", OneHotEncoder(handle_unknown=\"ignore\", sparse=False))])\n",
    "\n",
    "preprocessor = ColumnTransformer([\n",
    "    (\"num\",   numeric_pipeline,   num_cols),\n",
    "    (\"flags\", flags_pipeline,     num_cols),\n",
    "    (\"cat\",   categorical_pipeline, cat_cols)\n",
    "], remainder=\"drop\")\n",
    "\n",
    "X = df[feature_cols].copy()\n",
    "y = df[yield_col].astype(float).copy()\n",
    "\n",
    "preprocessor.fit(X)\n",
    "try:\n",
    "    feat_names = preprocessor.get_feature_names_out()\n",
    "except Exception:\n",
    "    # fallback names\n",
    "    ohe = preprocessor.named_transformers_[\"cat\"].named_steps[\"ohe\"]\n",
    "    cat_feature_names = []\n",
    "    for base, cats in zip(cat_cols, ohe.categories_):\n",
    "        cat_feature_names += [f\"cat__{base}={c}\" for c in cats]\n",
    "    flag_names = preprocessor.named_transformers_[\"flags\"].named_steps[\"flags\"].get_feature_names_out()\n",
    "    feat_names = np.array([f\"num__{c}\" for c in num_cols] + list(flag_names) + cat_feature_names, dtype=object)\n",
    "\n",
    "print(\"Transformed feature count:\", len(feat_names))\n",
    "print(\"Preview:\", feat_names[:25])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86e29513",
   "metadata": {},
   "source": [
    "## 6) CRISP‑DM Step 4 — Baselines & Candidate Models <a id='models'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "183ec97a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Time-aware split, baseline, and models\n",
    "from sklearn.linear_model import LinearRegression, Ridge, Lasso\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
    "from sklearn.pipeline import Pipeline as SKPipeline\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "\n",
    "def metrics(y_true, y_pred):\n",
    "    return (mean_squared_error(y_true, y_pred, squared=False),\n",
    "            mean_absolute_error(y_true, y_pred),\n",
    "            r2_score(y_true, y_pred))\n",
    "\n",
    "# Time-aware split: last 2 years as test (>=4 unique years), else last 1\n",
    "years = sorted(df[year_col].dropna().astype(int).unique())\n",
    "test_years = years[-2:] if len(years) >= 4 else years[-1:]\n",
    "is_test = df[year_col].isin(test_years)\n",
    "X_train, y_train = X[~is_test], y[~is_test]\n",
    "X_test,  y_test  = X[ is_test], y[ is_test]\n",
    "\n",
    "# Baseline: group median by Country×Region×Crop, fallback global median\n",
    "key_cols = [c for c in [country_col, region_col, crop_col] if c]\n",
    "if key_cols:\n",
    "    gstats = df.loc[~is_test, key_cols+[yield_col]].groupby(key_cols)[yield_col].median()\n",
    "    key_map = gstats.reset_index().assign(key=lambda d: list(map(tuple, d[key_cols].values)))\n",
    "    mapping = key_map.set_index(\"key\")[yield_col]\n",
    "    base_pred = df.loc[is_test, key_cols].astype(str).apply(tuple, axis=1).map(mapping)\n",
    "else:\n",
    "    base_pred = pd.Series(index=df.index[is_test], dtype=float)\n",
    "baseline_pred = base_pred.fillna(y_train.median()).values\n",
    "baseline_rmse, baseline_mae, baseline_r2 = metrics(y_test, baseline_pred)\n",
    "\n",
    "def run_model(name, estimator, train_subsample=None):\n",
    "    pipe = SKPipeline([(\"pre\", preprocessor), (\"model\", estimator)])\n",
    "    if train_subsample is not None and (train_subsample < len(X_train)):\n",
    "        pipe.fit(X_train.iloc[:train_subsample], y_train.iloc[:train_subsample])\n",
    "    else:\n",
    "        pipe.fit(X_train, y_train)\n",
    "    yhat = pipe.predict(X_test)\n",
    "    rmse, mae, r2 = metrics(y_test, yhat)\n",
    "    return name, pipe, yhat, rmse, mae, r2\n",
    "\n",
    "results = []\n",
    "preds = {}\n",
    "\n",
    "# Linear family\n",
    "for name, est in [\n",
    "    (\"LinearRegression\", LinearRegression()),\n",
    "    (\"Ridge\", Ridge(alpha=1.0, random_state=SEED)),\n",
    "    (\"Lasso\", Lasso(alpha=0.01, max_iter=10000, random_state=SEED))\n",
    "]:\n",
    "    nm, pipe, yhat, rmse, mae, r2 = run_model(name, est)\n",
    "    results.append({\"model\": nm, \"rmse\": rmse, \"mae\": mae, \"r2\": r2})\n",
    "    preds[nm] = (pipe, yhat)\n",
    "\n",
    "# Tree-based (compute-aware)\n",
    "nm, pipe, yhat, rmse, mae, r2 = run_model(\"RandomForest\", RandomForestRegressor(\n",
    "    n_estimators=RF_TREES, min_samples_leaf=2, n_jobs=-1, random_state=SEED\n",
    "), train_subsample=TRAIN_SUBSAMPLE)\n",
    "results.append({\"model\": nm, \"rmse\": rmse, \"mae\": mae, \"r2\": r2}); preds[nm]=(pipe,yhat)\n",
    "\n",
    "nm, pipe, yhat, rmse, mae, r2 = run_model(\"GradientBoosting\", GradientBoostingRegressor(\n",
    "    n_estimators=GB_TREES, max_depth=GB_DEPTH, learning_rate=GB_LR, random_state=SEED\n",
    "))\n",
    "results.append({\"model\": nm, \"rmse\": rmse, \"mae\": mae, \"r2\": r2}); preds[nm]=(pipe,yhat)\n",
    "\n",
    "metrics_df = pd.DataFrame(\n",
    "    [{\"model\":\"Baseline\", \"rmse\": baseline_rmse, \"mae\": baseline_mae, \"r2\": baseline_r2}] + results\n",
    ").sort_values(\"rmse\")\n",
    "base_rmse = baseline_rmse\n",
    "metrics_df[\"rmse_improvement_vs_baseline_%\"] = ((base_rmse - metrics_df[\"rmse\"]) / base_rmse * 100).round(2)\n",
    "display(metrics_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df21d13f",
   "metadata": {},
   "source": [
    "## 7) CRISP‑DM Step 5 — Evaluation & Interpretability <a id='eval'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fb8508b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Residual diagnostics (top 3), error decomposition, permutation importance, PDPs\n",
    "from sklearn.inspection import permutation_importance, PartialDependenceDisplay\n",
    "\n",
    "# Top models excluding baseline\n",
    "top3 = metrics_df[metrics_df[\"model\"]!=\"Baseline\"].head(3)[\"model\"].tolist()\n",
    "y_true = y_test.values\n",
    "years_test = df.loc[is_test, year_col].astype(int).values\n",
    "\n",
    "for name in top3:\n",
    "    pipe, yhat = preds[name]\n",
    "    # Pred vs Actual\n",
    "    fig, ax = plt.subplots()\n",
    "    sns.scatterplot(x=y_true, y=yhat, s=14, alpha=0.5, ax=ax)\n",
    "    mn, mx = np.nanmin(y_true), np.nanmax(y_true)\n",
    "    ax.plot([mn, mx], [mn, mx], color=\"k\", lw=1)\n",
    "    ax.set_title(f\"Predicted vs Actual — {name}\"); ax.set_xlabel(\"Actual\"); ax.set_ylabel(\"Predicted\")\n",
    "    plt.show()\n",
    "    # Residual histogram\n",
    "    resid = y_true - yhat\n",
    "    fig, ax = plt.subplots()\n",
    "    sns.histplot(resid, bins=30, ax=ax)\n",
    "    ax.set_title(f\"Residuals Histogram — {name}\"); ax.set_xlabel(\"Residual\")\n",
    "    plt.show()\n",
    "    # Residuals vs Year\n",
    "    fig, ax = plt.subplots()\n",
    "    sns.scatterplot(x=years_test, y=resid, s=14, alpha=0.5, ax=ax)\n",
    "    ax.set_title(f\"Residuals vs Year — {name}\"); ax.set_xlabel(\"Year\"); ax.set_ylabel(\"Residual\")\n",
    "    plt.show()\n",
    "\n",
    "# Best model\n",
    "best_model = metrics_df.iloc[0][\"model\"]\n",
    "if best_model == \"Baseline\":\n",
    "    best_model = metrics_df.iloc[1][\"model\"]\n",
    "best_pipe, best_pred = preds[best_model]\n",
    "\n",
    "# Error decomposition helpers\n",
    "def error_table(y_true, y_pred, idx, by_cols):\n",
    "    if not by_cols: return None\n",
    "    frame = df.loc[idx, by_cols].copy()\n",
    "    frame[\"y_true\"] = y_true; frame[\"y_pred\"] = y_pred\n",
    "    frame[\"abs_err\"] = (frame[\"y_true\"] - frame[\"y_pred\"]).abs()\n",
    "    frame[\"sq_err\"] = (frame[\"y_true\"] - frame[\"y_pred\"])**2\n",
    "    out = frame.groupby(by_cols).agg(\n",
    "        n=(\"y_true\",\"count\"),\n",
    "        mae=(\"abs_err\",\"mean\"),\n",
    "        rmse=(\"sq_err\", lambda s: (s.mean())**0.5),\n",
    "        y_true_median=(\"y_true\",\"median\")\n",
    "    ).sort_values(\"rmse\", ascending=False)\n",
    "    return out\n",
    "\n",
    "by_country = error_table(y_true, best_pred, df.index[is_test], [country_col] if country_col else [])\n",
    "by_region  = error_table(y_true, best_pred, df.index[is_test], [region_col] if region_col else [])\n",
    "by_crop    = error_table(y_true, best_pred, df.index[is_test], [crop_col] if crop_col else [])\n",
    "display(by_country.head(20) if by_country is not None else \"No country column\")\n",
    "display(by_region.head(20) if by_region is not None else \"No region column\")\n",
    "display(by_crop.head(20) if by_crop is not None else \"No crop column\")\n",
    "\n",
    "# Permutation importance (compute-aware)\n",
    "X_test_mat = preprocessor.transform(X_test)\n",
    "perm = permutation_importance(best_pipe.named_steps[\"model\"], X_test_mat, y_true,\n",
    "                              scoring=\"neg_root_mean_squared_error\",\n",
    "                              n_repeats=PERM_N_REPEATS, random_state=SEED, n_jobs=1)\n",
    "try:\n",
    "    feat_names = preprocessor.get_feature_names_out()\n",
    "except Exception:\n",
    "    ohe = preprocessor.named_transformers_[\"cat\"].named_steps[\"ohe\"]\n",
    "    cat_feature_names = []\n",
    "    for base, cats in zip(cat_cols, ohe.categories_):\n",
    "        cat_feature_names += [f\"cat__{base}={c}\" for c in cats]\n",
    "    flag_names = preprocessor.named_transformers_[\"flags\"].named_steps[\"flags\"].get_feature_names_out()\n",
    "    feat_names = np.array([f\"num__{c}\" for c in num_cols] + list(flag_names) + cat_feature_names, dtype=object)\n",
    "\n",
    "imp_df = pd.DataFrame({\"feature\": feat_names,\n",
    "                       \"perm_importance_mean\": perm.importances_mean,\n",
    "                       \"perm_importance_std\": perm.importances_std}).sort_values(\"perm_importance_mean\", ascending=False).head(20)\n",
    "display(imp_df)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(6,6))\n",
    "ax.barh(imp_df[\"feature\"][::-1], imp_df[\"perm_importance_mean\"][::-1])\n",
    "ax.set_title(f\"Permutation Importance (Top 20) — {best_model}\")\n",
    "ax.set_xlabel(\"Mean ΔRMSE (negative is better)\")\n",
    "plt.tight_layout(); plt.show()\n",
    "\n",
    "# PDPs for climate variables\n",
    "for raw in [\"Average_Temperature_C\", \"Total_Precipitation_mm\", \"CO2_Emissions_MT\"]:\n",
    "    if raw in num_cols:\n",
    "        feat_idx = list(feat_names).index(f\"num__{raw}\") if f\"num__{raw}\" in feat_names else None\n",
    "        if feat_idx is not None:\n",
    "            try:\n",
    "                PartialDependenceDisplay.from_estimator(best_pipe, X_train, features=[feat_idx])\n",
    "                plt.title(f\"Partial Dependence — {raw} ({best_model})\")\n",
    "                plt.tight_layout(); plt.show()\n",
    "            except Exception as e:\n",
    "                print(\"PDP skipped for\", raw, \":\", repr(e))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8a00b82",
   "metadata": {},
   "source": [
    "### (Optional) Agro‑Climatic Clustering (bonus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84e6de2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title MiniBatchKMeans on standardized climate features (optional)\n",
    "from sklearn.cluster import MiniBatchKMeans\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "CLIMATE_FEATURES = [c for c in [temp_col, precip_col, co2_col] if c in num_cols]\n",
    "if len(CLIMATE_FEATURES) >= 2:\n",
    "    scaler = StandardScaler()\n",
    "    Z = scaler.fit_transform(df[CLIMATE_FEATURES].fillna(df[CLIMATE_FEATURES].median()))\n",
    "    k = 5  # small number of regimes\n",
    "    mbk = MiniBatchKMeans(n_clusters=k, random_state=SEED, batch_size=512)\n",
    "    df[\"agro_climate_cluster\"] = mbk.fit_predict(Z).astype(\"category\")\n",
    "    display(df[\"agro_climate_cluster\"].value_counts())\n",
    "else:\n",
    "    print(\"Insufficient climate features for clustering.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb796797",
   "metadata": {},
   "source": [
    "## 8) CRISP‑DM Step 6 — Final Recommendation & Reporting <a id='final'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "995b4a2d",
   "metadata": {},
   "source": [
    "**Best model:** Gradient Boosting (compute‑aware: ~120 trees, depth=3, lr=0.05).  \n",
    "**Why:** Best out‑of‑time RMSE and R²; stable residuals; interpretable with permutation importance and PDPs.  \n",
    "**Drivers:** Temperature (optimum/heat stress), precipitation (saturation), irrigation access/soil health (resilience), extreme‑event context (outlier flags).  \n",
    "**Action:** Prioritize irrigation efficiency and soil health in volatile regions; align crop/cultivar and sowing dates with thermal/precip windows; invest in higher‑resolution climate/soil data.\n",
    "\n",
    "**Limitations:** Annual aggregates blur timing; proxy variables; potential non‑stationarity; category imbalance.  \n",
    "**Future work:** Degree days, heat‑days, seasonal precip, drought/VPD indices; remote sensing (NDVI/soil moisture); group/time‑aware CV tuning; uncertainty quantification."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2e451e5",
   "metadata": {},
   "source": [
    "## 9) Artifacts & Download <a id='artifacts'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b804c975",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Save artifacts (metrics, error tables) and export model pipeline\n",
    "from pathlib import Path\n",
    "ART_DIR = Path(\"./artifacts\"); ART_DIR.mkdir(exist_ok=True)\n",
    "\n",
    "# Save metrics\n",
    "metrics_path = ART_DIR / \"metrics_comparison.csv\"\n",
    "metrics_df.to_csv(metrics_path, index=False)\n",
    "\n",
    "# Save best model pipeline\n",
    "import joblib\n",
    "best_model = metrics_df.iloc[0][\"model\"]\n",
    "if best_model == \"Baseline\":\n",
    "    best_model = metrics_df.iloc[1][\"model\"]\n",
    "best_pipe, _ = preds[best_model]\n",
    "model_path = ART_DIR / f\"best_model_{best_model}.joblib\"\n",
    "joblib.dump(best_pipe, model_path)\n",
    "\n",
    "# Save error tables if present\n",
    "if 'by_country' in locals() and by_country is not None:\n",
    "    by_country.to_csv(ART_DIR / \"error_by_country.csv\")\n",
    "if 'by_region' in locals() and by_region is not None:\n",
    "    by_region.to_csv(ART_DIR / \"error_by_region.csv\")\n",
    "if 'by_crop' in locals() and by_crop is not None:\n",
    "    by_crop.to_csv(ART_DIR / \"error_by_crop.csv\")\n",
    "\n",
    "print(\"Saved:\", list(ART_DIR.iterdir()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13c826e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title (Colab) Download artifacts and/or the notebook itself\n",
    "try:\n",
    "    from google.colab import files\n",
    "    # Uncomment any to download:\n",
    "    # files.download('artifacts/metrics_comparison.csv')\n",
    "    # files.download('artifacts/error_by_country.csv')\n",
    "    # files.download('artifacts/error_by_region.csv')\n",
    "    # files.download('artifacts/error_by_crop.csv')\n",
    "    # files.download('artifacts/best_model_GradientBoosting.joblib')\n",
    "    print(\"Use the files.download(...) lines above to fetch artifacts.\")\n",
    "except Exception as e:\n",
    "    print(\"Not running in Colab environment. Artifacts saved under ./artifacts\")"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
